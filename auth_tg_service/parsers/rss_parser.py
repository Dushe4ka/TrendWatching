import aiohttp
import asyncio
import feedparser
import logging
from .utils import retry_on_failure, decode_if_bytes, save_parsed_data

log = logging.getLogger(__name__)

async def fetch_rss_content(url, headers):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ RSS-–ª–µ–Ω—Ç—ã —Å —Ç–∞–π–º–∞—É—Ç–∞–º–∏"""
    timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get(url, headers=headers) as response:
            return await response.text()

async def parse_rss(url: str, category: str, parsed_data_collection, verbose=False):
    """
    –ü–∞—Ä—Å–∏—Ç RSS –ª–µ–Ω—Ç—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ MongoDB
    
    Args:
        url: URL RSS –ª–µ–Ω—Ç—ã
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        parsed_data_collection: MongoDB –∫–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        verbose: –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    log.info(f"–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {url}")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, text/xml;q=0.9, */*;q=0.8'
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
        content = await retry_on_failure(fetch_rss_content, url, headers, max_retries=2, delay=2)
        feed = feedparser.parse(content)
        
        if verbose:
            print("\n=== –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ===")
            print(f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π URL: {getattr(feed, 'href', url)}")
            print(f"–°—Ç–∞—Ç—É—Å: {getattr(feed, 'status', 'N/A')}")
            if hasattr(feed, 'bozo_exception') and feed.bozo_exception:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞: {type(feed.bozo_exception).__name__}: {feed.bozo_exception}")
            print(f"Content-Type: {feed.headers.get('content-type', 'N/A')}")
            print(f"–í–µ—Ä—Å–∏—è: {getattr(feed, 'version', 'N/A')}")
            print(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞: {getattr(feed, 'encoding', 'N/A')}")
            print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–Ω–∞–ª–∞: {feed.feed.get('title', 'N/A')}")
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞: {feed.feed.get('description', 'N/A')}")
            print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(feed.entries)}")
            if len(feed.entries) > 0:
                print("\n–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π –∑–∞–ø–∏—Å–∏:")
                first_entry = feed.entries[0]
                print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {first_entry.get('title', 'N/A')}")
                print(f"–°—Å—ã–ª–∫–∞: {first_entry.get('link', 'N/A')}")
                print(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {first_entry.get('published', 'N/A')}")
                print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è: {list(first_entry.keys())}")
            print("===========================\n")
            
        if feed.bozo and isinstance(feed.bozo_exception, feedparser.NonXMLContentType):
            log.error(f"–°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª –Ω–µ XML (RSS) –∫–æ–Ω—Ç–µ–Ω—Ç: {feed.headers.get('content-type', 'N/A')}")
            if verbose:
                print("–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞:")
                print(f"–ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤:\n{content[:200]}")
            return None
            
        if not feed.entries:
            log.warning("RSS-–ª–µ–Ω—Ç–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø–∏—Å–µ–π")
            return None
        
        saved_count = 0
        for entry in feed.entries:
            data = {
                "url": decode_if_bytes(entry.link),
                "title": decode_if_bytes(entry.get('title', '')),
                "description": decode_if_bytes(entry.get('description', '')),
                "content": decode_if_bytes(entry.get('content', [{}])[0].get('value', '')) if hasattr(entry, 'content') else '',
                "date": entry.get('published', ''),
                "category": category,
                "source_type": "rss",
                "source_url": url
            }
            
            if await save_parsed_data(data, parsed_data_collection):
                saved_count += 1
                print(f"üìÑ RSS: {data['title'][:100]}... | {data['url']}")
            
        log.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—à–µ–Ω–æ {saved_count} –∑–∞–ø–∏—Å–µ–π –∏–∑ {url}")
        return saved_count
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ RSS ({url}): {str(e)}")
        return None

async def test_rss_parser():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RSS –ø–∞—Ä—Å–µ—Ä–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å"""
    print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞ ===")
    url = input("–í–≤–µ–¥–∏—Ç–µ URL RSS-–ª–µ–Ω—Ç—ã: ").strip()
    category = input("–í–≤–µ–¥–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é: ").strip()
    verbose = input("–ü–æ–∫–∞–∑–∞—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é? (y/n): ").lower() == 'y'
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞:")
    result = await parse_rss(url, category, verbose)
    if result:
        print(f"\n–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—à–µ–Ω–æ {result} –∑–∞–ø–∏—Å–µ–π")
    else:
        print("\n–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π –∏–ª–∏ –ª–µ–Ω—Ç–∞ –ø—É—Å—Ç–∞")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("- –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å RSS-–ª–µ–Ω—Ç—ã (–æ—Ç–∫—Ä–æ–π—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ)")
        print("- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å URL")
        print("- –ù–∞–ª–∏—á–∏–µ –∑–∞–ø–∏—Å–µ–π –≤ –ª–µ–Ω—Ç–µ")

if __name__ == "__main__":
    asyncio.run(test_rss_parser())
